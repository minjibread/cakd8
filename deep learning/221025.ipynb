{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "754e5d31-0d40-41eb-b39c-b2ce9d519cbe",
   "metadata": {},
   "source": [
    "# 복습문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e9879-920a-4c9b-aa04-86729391e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW의 Feature Vectorizer 두 가지는>\n",
    "\n",
    "Count, TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47767d25-11e7-428e-8000-3c42d73151e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "말뭉치를 영어로 뭐라할까요?\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bb6090-1543-4b8a-8df4-c7ad4741ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "어간 추출을 영어로 하면?\n",
    "\n",
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cf9c8e-4ec4-4a65-9414-36bc3d420163",
   "metadata": {},
   "outputs": [],
   "source": [
    "표제어 추출을 영어로 하면?\n",
    "\n",
    "lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5711810-8454-40d6-9bc6-cdf0d40327ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "토큰화 두 가지는?\n",
    "\n",
    "문장 토큰화(sent_tokenize), 단어 토큰화(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d96698-1b6b-4b0f-8b44-ecf3c39996ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW 모델은 문서가 가지는 모든 단어를 문맥이나 순서를 무시한다? O/X\n",
    "\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29326d33-06ca-422a-a075-1123d49c4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "텍스트를 피처 벡터화한다. 워드 임베딩(word embedding : 단어를 끼워넣다 정도로 생각하면 된다)의 두 가지 방법은?\n",
    "\n",
    "BOW, word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ec553-f37a-4993-940c-f992b0fa854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "is, the, a 등 문장을 구성하는 필수 문법 요소지만, 문맥적으로 큰 의미가 없는 단어를 지칭하는 말은?\n",
    "\n",
    "stop words(스톱 워드) 또는 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8512b-4060-471f-bceb-3d613dac8b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('working') 이것의 출력물을 예측하면?\n",
    "\n",
    "work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34e21a-f86f-4ab0-8f70-5f985c6bbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW 형태를 가진 언어 모델의 피처 벡터화는 대부분 (희소, sparse) 행렬이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf01e5-f8e1-4cc0-97bf-87528d2caeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "희소행렬에서 압축해서 저장하는 두 가지 방법은?\n",
    "\n",
    "COO 형식, CSR 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362be33-622a-4ac6-86b6-09215fee3093",
   "metadata": {},
   "outputs": [],
   "source": [
    "연속된 n개의 단어를 하나의 토큰화 단위로 분리하는 것을 지칭하는 말은?\n",
    "\n",
    "n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b44960-5f89-4a2e-9592-9955c583523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "카운트 벡터화를 위해 CountVectorizer() 사용하기 전에 반드시 텍스트 전처리를 해야 하나? O/X\n",
    "\n",
    "X\n",
    "\n",
    "이유는?\n",
    "\n",
    "CountVectorizer 클래스에서 소문자 일괄 변환, 토큰화, 스톱 워드 필터링 등의 텍스트 전처리 기능을 포함하여 제공해 주기 때문에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c70adb-0320-4ab9-9e4f-622bce2cd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = np.array([[0,1,0,3],\n",
    "                  [1,2,0,0],\n",
    "                  [0,0,1,0],\n",
    "                  [0,2,0,0]])\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "COO 형식: \n",
    "data = np.array([1, 3, 1,2,1,2])\n",
    "row = np.array([0,0,1,1,2,3])\n",
    "col = np.array([1,3,0,1,2,1])\n",
    "\n",
    "print(sparse.coo_matrix((data, (row, col)), shape=(4, 4)).toarray())\n",
    "    \n",
    "CSR 방식:\n",
    "    \n",
    "data = [1, 3, 1, 2, 1, 2]\n",
    "row_index = np.array([0, 2, 4, 5, 6])\n",
    "col = [1, 3, 0, 1, 2, 1]\n",
    "\n",
    "print(sparse.csr_matrix((data, col, row_index)).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc78e79e-3a65-4393-8711-1b85cfcb20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "데이터 전처리 내용 및 순서는?\n",
    "\n",
    "- 클렌징\n",
    "- 토큰화\n",
    "- 스톱워드 제거/필터링/철자 수정\n",
    "- Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75256af-01c3-4d69-a323-7a3cbb582e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I decided, very early on, just to accept life unconditionally. I never expected it to do anything special for me, \\\n",
    "yet I seemed to accomplish far more than I had ever hoped. Most of the time it just happened to me without my ever seeking it.'\n",
    "\n",
    "1. 특수 문자를 제거해 주세요.\n",
    "\n",
    "\n",
    "\n",
    "2. 단어 토큰으로 만들어 주세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1360c3-8cf4-4604-90e4-014f9f69ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text1 = re.sub('[^a-zA-Z]', ' ', text)\n",
    "text1\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text1)\n",
    "word_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
